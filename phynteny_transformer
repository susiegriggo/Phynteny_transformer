#!/usr/bin/env python

# Standard library imports
import os
import pickle
import sys
import time

# Third-party library imports
import click
from importlib_resources import files
from loguru import logger
import numpy as np
import torch
from torch.utils.data import DataLoader

# BioPython imports
from Bio import Seq, SeqIO, SeqRecord
from Bio.SeqFeature import FeatureLocation, SeqFeature

# Local application imports
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
from src import format_data, model_onehot, predictor

__author__ = "Susanna Grigson"
__maintainer__ = "Susanna Grigson"
__license__ = "MIT"
__version__ = "0"
__email__ = "susie.grigson@flinders.edu.au"
__status__ = "development"

@click.command()
@click.argument("infile", type=click.Path(exists=True))
@click.option(
    "-o",
    "--out",
    type=click.STRING,
    default="phynteny",
    help="output directory",
    required=True,
)
@click.option(
    "--prefix",
    type=click.STRING,
    default="phynteny",
    help="output prefix",
    required=False,
)
@click.option(
    "--esm_model",
    default="facebook/esm2_t33_650M_UR50D",
    type=click.Choice([
        "facebook/esm2_t48_15B_UR50D",
        "facebook/esm2_t6_8M_UR50D",
        "facebook/esm2_t12_35M_UR50D",
        "facebook/esm2_t30_150M_UR50D",
        "facebook/esm2_t33_650M_UR50D"
    ], case_sensitive=True),
    help="Specify path to esm model if not the default",
    required=False,
)

@click.option(
    "-m", 
    "--models",
    type=click.Path(exists=True),
    help="Path to the models to use for predictions",
    default=os.path.join(os.path.dirname(__file__), 'phynteny_utils', 'models')
)

@click.option("-f", "--force", is_flag=True, help="Overwrite output directory")

# Model configuration options
@click.option('--input-dim', default=1280, help='Input dimension for the model.')
@click.option('--num-classes', default=9, help='Number of classes for the model.')
@click.option('--num-heads', default=8, help='Number of attention heads for the model.')
@click.option('--hidden-dim', default=512, help='Hidden dimension for the model.')
@click.option('--lstm-hidden-dim', default=512, help='LSTM hidden dimension for the model.')
@click.option('--no-lstm', is_flag=True, help='Specify if LSTM should not be used in the model.')
@click.option('--max-len', default=1500, help='Maximum length for the model.')
@click.option('--protein-dropout-rate', default=0.6, help='Dropout rate for protein features.')
@click.option('--attention', type=click.Choice(['absolute', 'relative', 'circular']), default='circular', help='Type of attention mechanism to use.')
@click.option('--positional-encoding-type', type=click.Choice(['fourier', 'sinusoidal']), default='sinusoidal', help='Type of positional encoding to use.')
@click.option('--pre-norm', is_flag=True, default=False, help='Use pre-normalization in transformer layers instead of post-normalization.')
@click.option('--progressive-dropout', is_flag=True, default=True, help='Enable progressive dropout for protein features.')
@click.option('--initial-dropout-rate', default=1.0, help='Initial dropout rate when using progressive dropout.')
@click.option('--final-dropout-rate', default=0.4, help='Final dropout rate when using progressive dropout.')
@click.option('--output-dim', default=None, type=int, help='Output dimension for the model. Defaults to num_classes if not specified.')
@click.option('--num-layers', default=2, help='Number of transformer layers.')
@click.option('--dropout', default=0.1, help='Dropout rate for the model.')
@click.option('--batch-size', default=128, help='Batch size for processing data.')
@click.option('--confidence-threshold', default=0.8, type=float, help='Confidence threshold for high-confidence predictions (0.0-1.0).')
@click.option('--debug', is_flag=True, help='Enable verbose debug logging to console.')

@click.version_option(version=__version__)

def main(infile, out, esm_model, models, force, prefix, input_dim, num_classes, num_heads, 
         hidden_dim, lstm_hidden_dim, no_lstm, max_len, protein_dropout_rate, attention, 
         positional_encoding_type, pre_norm, progressive_dropout, initial_dropout_rate, 
         final_dropout_rate, output_dim, num_layers, dropout, batch_size, confidence_threshold,
         debug):

    start_time = time.time()
    # instantiate output directory 
    format_data.instantiate_output_directory(out, force)
    
    # Configure logging based on debug flag
    if debug:
        # When debug mode is enabled:
        # 1. Always log DEBUG level to the log file
        # 2. Also show DEBUG level logs on the console
        logger.remove()  # Remove default handlers
        logger.add(sys.stderr, level="DEBUG")  # Add console handler with DEBUG level
        logger.add(out + "/phynteny.log", level="DEBUG")  # Add file handler
        logger.info("Debug mode enabled - showing verbose logs")
    else:
        # Normal mode: DEBUG to file, INFO to console
        logger.remove()  # Remove default handlers
        logger.add(sys.stderr, level="INFO")  # Add console handler with INFO level
        logger.add(out + "/phynteny.log", level="DEBUG")  # Add file handler
    
    # Check if the models directory exists
    if not os.path.exists(models):
        logger.error(f"Models directory does not exist: {models}")
        sys.exit(1)
    else:
        logger.info(f"Models directory exists: {models}")

    # read in needed info on PHROGS 
    category_dict, phrog_integer_category = format_data.read_annotations_information() # what this this method even doing? 

    # Determine whether to use LSTM based on the --no-lstm flag
    use_lstm = not no_lstm

    # preamble 
    logger.info(f'Starting Phynteny transformer v{__version__}') 
    logger.info(f'Command executed: in={infile}, out={out}, esm_model={esm_model}, models={models}, force={force}')
    logger.info(f'Repository homepage is https://github.com/susiegriggo/Phynteny_transformer/tree/main')
    logger.info(f"Written by {__author__}: {__email__}")

    # read in genbank file
    gb_dict = format_data.read_genbank_file(infile, phrog_integer_category) # what categories the genes belong to is missing 
    logger.info("Genbank file read")
    
    # Log initial data dimensions
    num_genomes = len(gb_dict)
    logger.info(f"Number of genomes loaded from GenBank: {num_genomes}")
    
    # Log number of genes in each genome
    for genome_id, genome_data in gb_dict.items():
        if isinstance(genome_data, dict) and 'sequence' in genome_data:
            sequences = genome_data.get('sequence', [])
            #logger.debug(f"Genome {genome_id} has {len(sequences)} genes")
        else:
            logger.warning(f"Genome {genome_id} does not have expected structure")

    # Extract fasta from thqe genbank files
    logger.info("Extracting fasta from genbank")
    data_keys = list(gb_dict.keys())
    fasta_out = os.path.join(out, f"{prefix}.fasta")
    records = [gb_dict.get(k).get("sequence") for k in data_keys]
    with open(fasta_out, "w") as output_handle:
        for r in records:
            SeqIO.write(r, output_handle, "fasta")
    output_handle.close()

    # Extract the embeddings from the outputted fasta files
    logger.info("Computing ESM embeddings")
    logger.info("... if step is being slow consider using GPU!")
    embeddings = format_data.extract_embeddings(
        fasta_out, out, model_name=esm_model
    )

    # Prepare data
    X, y = format_data.prepare_data(embeddings, gb_dict)
    
    # Check data structure alignment
    logger.info(f"Data preparation complete. Checking dimensions:")
    logger.info(f"Number of genomes in gb_dict: {len(gb_dict)}")
    logger.info(f"Number of genomes in X: {len(X)}")
    logger.info(f"Number of genomes in y: {len(y)}")
    
    # Check if any genomes were dropped during data preparation
    if len(X) != len(gb_dict) or len(y) != len(gb_dict):
        missing_in_X = set(gb_dict.keys()) - set(X.keys())
        missing_in_y = set(gb_dict.keys()) - set(y.keys())
        logger.warning(f"Some genomes are missing in prepared data. Missing in X: {missing_in_X}, Missing in y: {missing_in_y}")
    
    # Check gene counts match between gb_dict and X/y for each genome
    for genome_id in X.keys():
        if genome_id in gb_dict:
            genome_data = gb_dict[genome_id]
            if isinstance(genome_data, dict) and 'sequence' in genome_data:
                num_genes_gb = len(genome_data['sequence'])
                num_genes_X = X[genome_id].shape[0]
                num_genes_y = y[genome_id].shape[0]
                if num_genes_gb != num_genes_X or num_genes_gb != num_genes_y:
                    logger.warning(f"Gene count mismatch for genome {genome_id}: gb_dict={num_genes_gb}, X={num_genes_X}, y={num_genes_y}")

    # Get the models 
    logger.info("Creating predictor object")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    p = predictor.Predictor(device=device)
    
    logger.info(f"Reading models from directory: {models}")
    logger.info(f"Model parameters: input_dim={input_dim}, num_classes={num_classes}, num_heads={num_heads}, "
                f"hidden_dim={hidden_dim}, lstm_hidden_dim={lstm_hidden_dim}, dropout={dropout}, "
                f"use_lstm={use_lstm}, max_len={max_len}, protein_dropout_rate={protein_dropout_rate}, "
                f"attention={attention}, positional_encoding_type={positional_encoding_type}, "
                f"pre_norm={pre_norm}, progressive_dropout={progressive_dropout}, "
                f"initial_dropout_rate={initial_dropout_rate}, final_dropout_rate={final_dropout_rate}, "
                f"output_dim={output_dim}, num_layers={num_layers}")
                
    p.read_models_from_directory(
        models, 
        input_dim=input_dim, 
        num_classes=num_classes, 
        num_heads=num_heads, 
        hidden_dim=hidden_dim, 
        lstm_hidden_dim=lstm_hidden_dim, 
        dropout=dropout, 
        use_lstm=use_lstm, 
        max_len=max_len, 
        protein_dropout_rate=protein_dropout_rate, 
        attention=attention, 
        positional_encoding_type=positional_encoding_type, 
        pre_norm=pre_norm, 
        progressive_dropout=progressive_dropout, 
        initial_dropout_rate=initial_dropout_rate, 
        final_dropout_rate=final_dropout_rate, 
        output_dim=output_dim,
        num_layers=num_layers
    )

    # Read the confidence_dict from a pickled file
    confidence_dict_path = os.path.join(models, "confidence_dict.pkl")
    if os.path.exists(confidence_dict_path):
        with open(confidence_dict_path, 'rb') as f:
            confidence_dict = pickle.load(f)
        logger.info("Loaded confidence dictionary")
    else:
        logger.error(f"Confidence dictionary file does not exist: {confidence_dict_path}")
        sys.exit(1)

    logger.info("Making predictions using batch inference")
    
    # Use the batch inference method
    all_scores = p.predict_inference(X, y, batch_size=batch_size)
    #logger.info(f'all_scores: {all_scores}')
    
    # Convert all_scores format to match what's expected by compute_confidence
    scores = [all_scores[key] for key in all_scores.keys()]
    
    logger.info(f"Number of valid scores collected: {len(scores)}")
    logger.info(f"Number of genomes in original data: {len(gb_dict)}")
    logger.info(f"Number of genomes in X: {len(X)}")
    
    # Verify that we got scores for all genomes
    if len(scores) != len(X):
        logger.warning(f"Score count mismatch: {len(scores)} scores vs {len(X)} genomes in X")
        # Identify which genomes are missing
        genome_ids = list(X.keys())
        if len(genome_ids) > len(scores):
            logger.warning(f"Missing scores for some genomes")

    logger.info("Calculating confidence scores")
    predictions, confidence_scores = p.compute_confidence(scores, confidence_dict, phrog_integer_category)
    
    # Check predictions and confidence scores dimensions
    logger.info(f"Data dimensions after prediction:")
    logger.info(f"Number of prediction arrays: {len(predictions)}")
    logger.info(f"Number of score arrays: {len(scores)}")
    logger.info(f"Number of confidence score arrays: {len(confidence_scores)}")
    
    # Check if predictions match the expected number of genomes and genes
    if len(predictions) != len(scores):
        logger.error(f"Prediction count mismatch: {len(predictions)} prediction arrays vs {len(scores)} score arrays")
    
    # Check gene count for each genome in predictions
    for i, (genome_id, genome_scores) in enumerate(zip(X.keys(), scores)):
        if genome_id in gb_dict:
            genome_data = gb_dict[genome_id]
            if isinstance(genome_data, dict) and 'sequence' in genome_data:
                num_genes_gb = len(genome_data['sequence'])
                num_predictions = len(predictions[i]) if i < len(predictions) else 0
                
                if num_genes_gb != num_predictions:
                    logger.warning(f"Gene prediction mismatch for genome {genome_id}: gb_dict={num_genes_gb}, predictions={num_predictions}")
    
    logger.info("Creating SeqRecord objects for GenBank output")
        
    logger.info(f"Writing predictions to genbank file")
    genbank_file = out + "/phynteny.gbk"

    # Create the categories map from confidence_dict
    categories_map = dict(zip(range(len(confidence_dict.keys())), confidence_dict.keys()))
    
    logger.info("Creating GenBank and table outputs")
    
    # Create ordered list of genome IDs for consistent indexing
    genome_ids = list(gb_dict.keys())
    logger.info(f"Writing GenBank and table data for {len(genome_ids)} genomes")
    logger.info(f"First 5 genome IDs: {genome_ids[:5] if len(genome_ids) >= 5 else genome_ids}")
    
    # Write predictions to GenBank file
    p.write_genbank(
        gb_dict, 
        genbank_file, 
        predictions, 
        scores, 
        confidence_scores,
        threshold=confidence_threshold,
        categories_map=categories_map
    )
    
    # Log final data structure info before generating the table
    logger.info("Final data structure check before table generation:")
    logger.info(f"Number of genomes in gb_dict: {len(gb_dict)}")
    logger.info(f"Number of prediction arrays: {len(predictions)}")
    logger.info(f"Number of score arrays: {len(scores)}")
    logger.info(f"Number of confidence score arrays: {len(confidence_scores)}")
    
    # Generate the table
    table_file = out + "/phynteny.tsv"
    format_data.generate_table(
        table_file, 
        gb_dict, 
        category_dict, 
        phrog_integer_category,
        predictions=predictions,
        scores=scores,
        confidence_scores=confidence_scores,
        categories_map=categories_map
    )
    
    logger.info("Predictions completed")
    end_time = time.time()
    elapsed_time = end_time - start_time
    logger.info(f"Total execution time: {elapsed_time:.2f} seconds")

if __name__ == "__main__":
    main()

